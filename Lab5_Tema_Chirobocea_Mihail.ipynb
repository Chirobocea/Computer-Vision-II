{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2u0yZqFCyVwy"
      },
      "source": [
        "#Laborator 5\n",
        "\n",
        "In cadrul acestui laborator vom implementa o solutie de detectie. Datasetul folosit se numeste [American Sign Language Letters Dataset](https://public.roboflow.com/object-detection/american-sign-language-letters/1/download/coco)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models"
      ],
      "metadata": {
        "id": "26ytMpctNMTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czyIhYt5gmUQ"
      },
      "outputs": [],
      "source": [
        "# !curl -L \"https://public.roboflow.com/ds/KamceLFGGS?key=dqp8HADMki\" > roboflow.zip; unzip -q roboflow.zip; rm roboflow.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eA7YWXQ2Jp1I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0d333b4-4921-4ce3-f069-e92132ee13ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "README.roboflow.txt  sample_data  test\ttrain  valid\n"
          ]
        }
      ],
      "source": [
        "! ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BVgY9gr2sbr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff89f081-c98c-4f45-e5d2-f9e52ae03907"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_annotations.coco.json\n"
          ]
        }
      ],
      "source": [
        "!ls train | grep \"json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOA4ted_hHdB"
      },
      "source": [
        "## Crearea Dataloader-ului\n",
        "\n",
        "In continuare, pentru a incarca date, folosim un obiect de tipul torch.utils.data.Dataset. Acesta are 3 metode importante:\n",
        "\n",
        "```\n",
        "__init__()\n",
        "__len__()\n",
        "__get_item__()\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Vhpi3uC55N6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.patches as patches\n",
        "import matplotlib.pyplot as plt\n",
        "import torch as t\n",
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms.functional import to_tensor, normalize\n",
        "import random\n",
        "from pycocotools.coco import COCO\n",
        "import os\n",
        "\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "class HandsDataset(Dataset):\n",
        "  def __init__(self, coco_root, coco_annos, coco_imgs, img_size=(320, 320)):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        dataset_lines (array): array of strings of form \"{image_path}|{rectangle_coordinates}\".\n",
        "        width (int): target image width.\n",
        "        height (int): target image height.\n",
        "    \"\"\"\n",
        "    self.coco_root = coco_root\n",
        "    self.coco_annos = coco_annos\n",
        "    self.coco_imgs = coco_imgs\n",
        "\n",
        "    self.coco_anno_file = os.path.join(coco_root, coco_annos)\n",
        "    self.coco_imgs_dir = os.path.join(coco_root, coco_imgs)\n",
        "\n",
        "    self.coco = COCO(self.coco_anno_file)\n",
        "\n",
        "    self.img_size = img_size\n",
        "\n",
        "    self.init_dataset()\n",
        "\n",
        "  def init_dataset(self):\n",
        "    self.cat_ids = self.coco.getCatIds()\n",
        "\n",
        "    self.img_ids = self.coco.getImgIds()\n",
        "    self.ann_ids = self.coco.getAnnIds(self.img_ids)\n",
        "\n",
        "    print(\"Dataset size {}\".format(len(self.img_ids)))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.img_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_meta = self.coco.loadImgs(self.img_ids[idx])[0]\n",
        "    img_path = os.path.join(self.coco_imgs_dir, img_meta['file_name'])\n",
        "    img = Image.open(img_path)\n",
        "    ann_id = self.coco.getAnnIds(self.img_ids[idx])\n",
        "    annos = self.coco.loadAnns(ann_id)[0]\n",
        "\n",
        "    original_width, original_height = img.size\n",
        "\n",
        "    img = img.resize(self.img_size)\n",
        "    img = np.array(img)\n",
        "\n",
        "    if len(img.shape) == 2:\n",
        "      img = np.expand_dims(img, axis=2)\n",
        "      img = np.repeat(img, 3, axis=2)\n",
        "\n",
        "    img = to_tensor(img)\n",
        "\n",
        "    bbox = annos['bbox']  # box is xywh\n",
        "    cat_id = annos['category_id']\n",
        "\n",
        "    x1, y1, w, h = bbox\n",
        "    x2, y2 = x1 + w, y1 + h\n",
        "\n",
        "    # x1 = x1 / original_width * self.img_size[0]\n",
        "    # x2 = x2 / original_width * self.img_size[0]\n",
        "    # y1 = y1 / original_height * self.img_size[1]\n",
        "    # y2 = y2 / original_height * self.img_size[1]\n",
        "\n",
        "    x1 = x1 / original_width\n",
        "    x2 = x2 / original_width\n",
        "    y1 = y1 / original_height\n",
        "    y2 = y2 / original_height\n",
        "\n",
        "    coordinates = np.array([x1, y1, x2, y2])\n",
        "    coordinates =  coordinates.astype(np.float32)\n",
        "\n",
        "    return img, coordinates, cat_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkijQD0uK85D"
      },
      "source": [
        "Construire Dataset si vizualizare date.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGHfd229hRyR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c3e05de-1b1f-478a-e4ac-b13f2279ffee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.10s)\n",
            "creating index...\n",
            "index created!\n",
            "Dataset size 1512\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Dataset size 144\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import clear_output\n",
        "import time\n",
        "\n",
        "coco_root = \"\"\n",
        "coco_anno_train = os.path.join(coco_root, \"train/_annotations.coco.json\")\n",
        "coco_imgs_train = os.path.join(coco_root, \"train\")\n",
        "\n",
        "coco_anno_valid = os.path.join(coco_root, \"valid/_annotations.coco.json\")\n",
        "coco_imgs_valid = os.path.join(coco_root, \"valid\")\n",
        "\n",
        "dataset_train = HandsDataset(coco_root, coco_anno_train, coco_imgs_train, img_size=(224, 224))\n",
        "train_loader = DataLoader(dataset_train, batch_size=16, shuffle=True, num_workers=1)\n",
        "\n",
        "dataset_valid = HandsDataset(coco_root, coco_anno_valid, coco_imgs_valid, img_size=(224, 224))\n",
        "valid_loader = DataLoader(dataset_valid, batch_size=1, shuffle=False, num_workers=1) # keep batch_size=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UibCKTqfBfv8"
      },
      "outputs": [],
      "source": [
        "# see_examples = 10\n",
        "# for i, (imgs, coordinates, cat_id) in enumerate(train_loader):\n",
        "#     clear_output(wait=True)\n",
        "#     imgs = np.transpose(imgs, (0, 2, 3, 1))\n",
        "#     print(imgs.shape)\n",
        "\n",
        "#     plt.imshow(imgs[0])\n",
        "\n",
        "#     x1, y1, x2, y2 = coordinates[0]\n",
        "#     x1 = x1 * dataset_train.img_size[1]\n",
        "#     y1 = y1 * dataset_train.img_size[0]\n",
        "#     x2 = x2 * dataset_train.img_size[1]\n",
        "#     y2 = y2 * dataset_train.img_size[0]\n",
        "\n",
        "#     rect = patches.Rectangle((x1,y1),x2-x1+1,y2-y1+1,linewidth=1,edgecolor='r',facecolor='none')\n",
        "#     plt.gca().add_patch(rect)\n",
        "#     plt.show()\n",
        "\n",
        "#     if i >= see_examples - 1:\n",
        "#       break\n",
        "#     time.sleep(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnV6PIC1kQMi"
      },
      "source": [
        "# Crearea unei retele neurale convolutionale\n",
        "\n",
        "### Cerinte\n",
        "* Creati o arhitectura de retea neuronala convolutionala pentru regresie pe cele 4 coordonate alte imaginilor din dataset.\n",
        "* Punctaj: 7 puncte pentru o retea cu rezultate *bune*.\n",
        "\n",
        "#### Hint\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "*   Numarul de exemple de antrenare este prea mic pentru a antrena o retea \"from scratch\". Folositi o retea prea-antrenata pe ImagetNet, care a invatat deja sa recunoasca trasaturi utile pentru detectia de obiecte. Arhitectura recomandata este ResNet18 din Pytorch. Alte arhitecturi pot fi incercate.\n",
        "*   La final trebuie utilizata o functie de activare care acopera [0,1]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK0Z9NeYTghv"
      },
      "source": [
        "### Definirea obiectelor folosite in timpul antrenarii\n",
        "  * Numarul de epoci\n",
        "  * Retea\n",
        "  * Optimizator\n",
        "  * Functie de loss\n",
        "\n",
        "Experimentati cu valorile hiper-parametrilor de mai sus astfel incat reteaua sa invete *bine*."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.resnet = models.resnet18(pretrained=True).cuda()\n",
        "        self.regression = nn.Sequential(\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(1000, 4),\n",
        "            nn.Sigmoid(),\n",
        "        ).cuda()\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.resnet(x)\n",
        "        x = self.regression(x)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "xLJD_wRgLW0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVixJ_VAO4qH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5c596bc-b67d-4687-9935-ce91db9d0d13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "epochs = 30\n",
        "network = Net()\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = optim.Adam(network.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAnUsWYWodb4"
      },
      "source": [
        "## Definirea functiei de antrenare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9MTYanoMZ8H"
      },
      "outputs": [],
      "source": [
        "def bb_intersection_over_union(boxA, boxB):\n",
        "\t# determine the (x, y)-coordinates of the intersection rectangle\n",
        "  # import pdb; pdb.set_trace()\n",
        "  xA = max(boxA[0], boxB[0])\n",
        "  yA = max(boxA[1], boxB[1])\n",
        "  xB = min(boxA[2], boxB[2])\n",
        "  yB = min(boxA[3], boxB[3])\n",
        "\t# compute the area of intersection rectangle\n",
        "  interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
        "  # compute the area of both the prediction and ground-truth\n",
        "  # rectangles\n",
        "  boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
        "  boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
        "  # compute the intersection over union by taking the intersection\n",
        "  # area and dividing it by the sum of prediction + ground-truth\n",
        "  # areas - the interesection area\n",
        "  iou = interArea / float(boxAArea + boxBArea - interArea)\n",
        "  # return the intersection over union value\n",
        "  return iou\n",
        "\n",
        "def train_fn(epochs: int, train_loader: DataLoader, test_loader: DataLoader, \n",
        "             net: torch.nn.Module, loss_fn: torch.nn.Module, optimizer: optim.Optimizer):\n",
        "  # Iteram prin numarul de epoci\n",
        "  for e in range(epochs):\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "\n",
        "    # Iteram prin fiecare exemplu din dataset\n",
        "    net.train()\n",
        "    for idx, (images, labels, _) in enumerate(train_loader):\n",
        "\n",
        "      images = images.cuda()\n",
        "      # Aplicam reteaua neurala pe imaginile de intrare\n",
        "      out = net(images)\n",
        "      # Aplicam functia cost pe iesirea retelei neurale si pe adnotarile imaginilor \n",
        "      loss = loss_fn(out, labels.cuda())\n",
        "      # Aplicam algoritmul de back-propagation\n",
        "      loss.backward()\n",
        "      # Facem pasul de optimizare, pentru a aplica gradientii pe parametrii retelei\n",
        "      optimizer.step()\n",
        "      # Apelam functia zero_grad() pentru a uita gradientii de la iteratie curenta\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      train_loss += loss.item()*images.size(0)\n",
        "\n",
        "    \n",
        "    # Caluculul acuratetii\n",
        "    count = len(test_loader)\n",
        "    IOU_list = []\n",
        "    net.eval()\n",
        "\n",
        "    for test_image, box_gt, _ in test_loader:\n",
        "      test_image = test_image.cuda()\n",
        "      bbox_pred = net(test_image)\n",
        "\n",
        "      loss = loss_fn(bbox_pred, box_gt.cuda())\n",
        "\n",
        "      bbox_pred = bbox_pred.detach().cpu().numpy()\n",
        "\n",
        "      IOU_list.append(bb_intersection_over_union(bbox_pred[0], box_gt[0]))\n",
        "\n",
        "      valid_loss += loss.item()*test_image.size(0)\n",
        "\n",
        "    train_loss = train_loss/len(train_loader.sampler)\n",
        "    valid_loss = valid_loss/len(valid_loader.sampler)\n",
        "\n",
        "    print(f\"Average train loss : {train_loss}\")\n",
        "    print(f\"Average valid loss : {valid_loss}\")\n",
        "\n",
        "    IOU_list = np.array(IOU_list)\n",
        "    IOU_list_50 = (IOU_list > 0.5).sum()\n",
        "    IOU_list_75 = (IOU_list > 0.75).sum()\n",
        "    IOU_list_90 = (IOU_list > 0.90).sum()\n",
        "\n",
        "    print(\"Acuratetea IOU 50% la finalul epocii {} este {:.2f}%\".format(e, (IOU_list_50 / count)*100))\n",
        "    print(\"Acuratetea IOU 75% la finalul epocii {} este {:.2f}%\".format(e, (IOU_list_75 / count)*100))\n",
        "    print(\"Acuratetea IOU 90% la finalul epocii {} este {:.2f}%\".format(e, (IOU_list_90 / count)*100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqUwOWmDMpqQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71336280-8334-41aa-f332-8bc3074bccdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average train loss : 0.01624277951048007\n",
            "Average valid loss : 0.008132773387842462\n",
            "Acuratetea IOU 50% la finalul epocii 0 este 100.00%\n",
            "Acuratetea IOU 75% la finalul epocii 0 este 90.28%\n",
            "Acuratetea IOU 90% la finalul epocii 0 este 13.19%\n",
            "Average train loss : 0.004835821693102834\n",
            "Average valid loss : 0.005545788948135548\n",
            "Acuratetea IOU 50% la finalul epocii 1 este 100.00%\n",
            "Acuratetea IOU 75% la finalul epocii 1 este 97.22%\n",
            "Acuratetea IOU 90% la finalul epocii 1 este 25.69%\n",
            "Average train loss : 0.0029441589432911424\n",
            "Average valid loss : 0.004313212234137609\n",
            "Acuratetea IOU 50% la finalul epocii 2 este 100.00%\n",
            "Acuratetea IOU 75% la finalul epocii 2 este 97.92%\n",
            "Acuratetea IOU 90% la finalul epocii 2 este 40.28%\n",
            "Average train loss : 0.002361289613569776\n",
            "Average valid loss : 0.003933941563142677\n",
            "Acuratetea IOU 50% la finalul epocii 3 este 100.00%\n",
            "Acuratetea IOU 75% la finalul epocii 3 este 97.92%\n",
            "Acuratetea IOU 90% la finalul epocii 3 este 37.50%\n",
            "Average train loss : 0.0019929609513994325\n",
            "Average valid loss : 0.003105655730981501\n",
            "Acuratetea IOU 50% la finalul epocii 4 este 100.00%\n",
            "Acuratetea IOU 75% la finalul epocii 4 este 100.00%\n",
            "Acuratetea IOU 90% la finalul epocii 4 este 47.92%\n",
            "Average train loss : 0.0015316664366908962\n",
            "Average valid loss : 0.0042173938290943624\n",
            "Acuratetea IOU 50% la finalul epocii 5 este 100.00%\n",
            "Acuratetea IOU 75% la finalul epocii 5 este 98.61%\n",
            "Acuratetea IOU 90% la finalul epocii 5 este 35.42%\n",
            "Average train loss : 0.0014849363465266174\n",
            "Average valid loss : 0.002920718730213897\n",
            "Acuratetea IOU 50% la finalul epocii 6 este 100.00%\n",
            "Acuratetea IOU 75% la finalul epocii 6 este 99.31%\n",
            "Acuratetea IOU 90% la finalul epocii 6 este 54.17%\n",
            "Average train loss : 0.001333503696880249\n",
            "Average valid loss : 0.0026432178175734102\n",
            "Acuratetea IOU 50% la finalul epocii 7 este 100.00%\n",
            "Acuratetea IOU 75% la finalul epocii 7 este 100.00%\n",
            "Acuratetea IOU 90% la finalul epocii 7 este 63.19%\n",
            "Average train loss : 0.0011739196525590997\n",
            "Average valid loss : 0.0024495443372466574\n",
            "Acuratetea IOU 50% la finalul epocii 8 este 100.00%\n",
            "Acuratetea IOU 75% la finalul epocii 8 este 99.31%\n",
            "Acuratetea IOU 90% la finalul epocii 8 este 64.58%\n",
            "Average train loss : 0.0011117031274821669\n",
            "Average valid loss : 0.0026681652111114496\n",
            "Acuratetea IOU 50% la finalul epocii 9 este 100.00%\n",
            "Acuratetea IOU 75% la finalul epocii 9 este 100.00%\n",
            "Acuratetea IOU 90% la finalul epocii 9 este 56.94%\n",
            "Average train loss : 0.000988437787373427\n",
            "Average valid loss : 0.002460686088700944\n",
            "Acuratetea IOU 50% la finalul epocii 10 este 100.00%\n",
            "Acuratetea IOU 75% la finalul epocii 10 este 99.31%\n",
            "Acuratetea IOU 90% la finalul epocii 10 este 61.81%\n",
            "Average train loss : 0.0010304263664813584\n",
            "Average valid loss : 0.0024663451044438667\n",
            "Acuratetea IOU 50% la finalul epocii 11 este 100.00%\n",
            "Acuratetea IOU 75% la finalul epocii 11 este 100.00%\n",
            "Acuratetea IOU 90% la finalul epocii 11 este 61.11%\n",
            "Average train loss : 0.0010121860104075895\n",
            "Average valid loss : 0.001957836748589317\n",
            "Acuratetea IOU 50% la finalul epocii 12 este 100.00%\n",
            "Acuratetea IOU 75% la finalul epocii 12 este 100.00%\n",
            "Acuratetea IOU 90% la finalul epocii 12 este 68.75%\n",
            "Average train loss : 0.0009486955031875777\n",
            "Average valid loss : 0.002173557586375965\n",
            "Acuratetea IOU 50% la finalul epocii 13 este 100.00%\n",
            "Acuratetea IOU 75% la finalul epocii 13 este 100.00%\n",
            "Acuratetea IOU 90% la finalul epocii 13 este 64.58%\n",
            "Average train loss : 0.0008887300870218683\n",
            "Average valid loss : 0.001972597480643243\n",
            "Acuratetea IOU 50% la finalul epocii 14 este 100.00%\n",
            "Acuratetea IOU 75% la finalul epocii 14 este 100.00%\n",
            "Acuratetea IOU 90% la finalul epocii 14 este 73.61%\n",
            "Average train loss : 0.0007909986568693683\n",
            "Average valid loss : 0.00194964913700662\n",
            "Acuratetea IOU 50% la finalul epocii 15 este 100.00%\n",
            "Acuratetea IOU 75% la finalul epocii 15 este 100.00%\n",
            "Acuratetea IOU 90% la finalul epocii 15 este 71.53%\n",
            "Average train loss : 0.0007058863002048047\n",
            "Average valid loss : 0.002236953785061107\n",
            "Acuratetea IOU 50% la finalul epocii 16 este 100.00%\n",
            "Acuratetea IOU 75% la finalul epocii 16 este 100.00%\n",
            "Acuratetea IOU 90% la finalul epocii 16 este 70.14%\n",
            "Average train loss : 0.000718327023263863\n",
            "Average valid loss : 0.001733124015433734\n",
            "Acuratetea IOU 50% la finalul epocii 17 este 100.00%\n",
            "Acuratetea IOU 75% la finalul epocii 17 este 100.00%\n",
            "Acuratetea IOU 90% la finalul epocii 17 este 75.69%\n",
            "Average train loss : 0.0006818826206856304\n",
            "Average valid loss : 0.002084474120591848\n",
            "Acuratetea IOU 50% la finalul epocii 18 este 100.00%\n",
            "Acuratetea IOU 75% la finalul epocii 18 este 100.00%\n",
            "Acuratetea IOU 90% la finalul epocii 18 este 70.14%\n",
            "Average train loss : 0.0008290852173650391\n",
            "Average valid loss : 0.0020483338812482543\n",
            "Acuratetea IOU 50% la finalul epocii 19 este 100.00%\n",
            "Acuratetea IOU 75% la finalul epocii 19 este 99.31%\n",
            "Acuratetea IOU 90% la finalul epocii 19 este 70.14%\n",
            "Average train loss : 0.000644890366062995\n",
            "Average valid loss : 0.0019999346444213493\n",
            "Acuratetea IOU 50% la finalul epocii 20 este 100.00%\n",
            "Acuratetea IOU 75% la finalul epocii 20 este 99.31%\n",
            "Acuratetea IOU 90% la finalul epocii 20 este 77.78%\n",
            "Average train loss : 0.0006279683439061046\n",
            "Average valid loss : 0.001892584884343604\n",
            "Acuratetea IOU 50% la finalul epocii 21 este 100.00%\n",
            "Acuratetea IOU 75% la finalul epocii 21 este 99.31%\n",
            "Acuratetea IOU 90% la finalul epocii 21 este 76.39%\n",
            "Average train loss : 0.0006272854525521989\n",
            "Average valid loss : 0.001655998909983383\n",
            "Acuratetea IOU 50% la finalul epocii 22 este 100.00%\n",
            "Acuratetea IOU 75% la finalul epocii 22 este 99.31%\n",
            "Acuratetea IOU 90% la finalul epocii 22 este 79.17%\n",
            "Average train loss : 0.0005416681176352084\n",
            "Average valid loss : 0.0016839427307786536\n",
            "Acuratetea IOU 50% la finalul epocii 23 este 100.00%\n",
            "Acuratetea IOU 75% la finalul epocii 23 este 99.31%\n",
            "Acuratetea IOU 90% la finalul epocii 23 este 79.86%\n",
            "Average train loss : 0.0006064279088085252\n",
            "Average valid loss : 0.0018608109968327982\n",
            "Acuratetea IOU 50% la finalul epocii 24 este 100.00%\n",
            "Acuratetea IOU 75% la finalul epocii 24 este 99.31%\n",
            "Acuratetea IOU 90% la finalul epocii 24 este 77.78%\n",
            "Average train loss : 0.0005315463757758379\n",
            "Average valid loss : 0.0019597686391029535\n",
            "Acuratetea IOU 50% la finalul epocii 25 este 100.00%\n",
            "Acuratetea IOU 75% la finalul epocii 25 este 99.31%\n",
            "Acuratetea IOU 90% la finalul epocii 25 este 73.61%\n",
            "Average train loss : 0.000565714267650422\n",
            "Average valid loss : 0.0015693785522014675\n",
            "Acuratetea IOU 50% la finalul epocii 26 este 100.00%\n",
            "Acuratetea IOU 75% la finalul epocii 26 este 99.31%\n",
            "Acuratetea IOU 90% la finalul epocii 26 este 80.56%\n",
            "Average train loss : 0.0005810756634221112\n",
            "Average valid loss : 0.0017031303454094389\n",
            "Acuratetea IOU 50% la finalul epocii 27 este 100.00%\n",
            "Acuratetea IOU 75% la finalul epocii 27 este 99.31%\n",
            "Acuratetea IOU 90% la finalul epocii 27 este 78.47%\n",
            "Average train loss : 0.0005473307806323405\n",
            "Average valid loss : 0.0017283377825757877\n",
            "Acuratetea IOU 50% la finalul epocii 28 este 100.00%\n",
            "Acuratetea IOU 75% la finalul epocii 28 este 99.31%\n",
            "Acuratetea IOU 90% la finalul epocii 28 este 77.78%\n",
            "Average train loss : 0.0005359738044534372\n",
            "Average valid loss : 0.0018284945035197274\n",
            "Acuratetea IOU 50% la finalul epocii 29 este 100.00%\n",
            "Acuratetea IOU 75% la finalul epocii 29 este 99.31%\n",
            "Acuratetea IOU 90% la finalul epocii 29 este 78.47%\n"
          ]
        }
      ],
      "source": [
        "train_fn(epochs, train_loader, valid_loader, network, loss_fn, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMOQhLY5Folz"
      },
      "source": [
        "## Augmentare date\n",
        "\n",
        "O metoda de imbunatatire a performantei modelului este augmentarea setului de date. Aceste augmentari trebuie sa tina cont de natura taskului si de natura etichetelor.\n",
        "\n",
        "### Cerinta\n",
        "\n",
        "(3p) Modificati Dataset-ul precedent astfel incat sa augmenteze datele de antrenare cu o probabilitate aleasa de voi. Cum afecteaza acest lucru performanta modelului?\n",
        "\n",
        "### Bonus\n",
        "(2p) Dataloaderul intoarce si clasa fiecarui obiect de detectat (litera corespunzatoare fiecarui semn din limbajul Sign Language). Creati o arhitectura care prezice si acesta clasa, pe langa regresia pe box. Antrenati reteaua. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLiuxyDZhVNG"
      },
      "outputs": [],
      "source": [
        "class HandsDataset(Dataset):\n",
        "  def __init__(self, coco_root, coco_annos, coco_imgs, img_size=(320, 320), transform = None):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        dataset_lines (array): array of strings of form \"{image_path}|{rectangle_coordinates}\".\n",
        "        width (int): target image width.\n",
        "        height (int): target image height.\n",
        "    \"\"\"\n",
        "    self.coco_root = coco_root\n",
        "    self.coco_annos = coco_annos\n",
        "    self.coco_imgs = coco_imgs\n",
        "\n",
        "    self.coco_anno_file = os.path.join(coco_root, coco_annos)\n",
        "    self.coco_imgs_dir = os.path.join(coco_root, coco_imgs)\n",
        "\n",
        "    self.coco = COCO(self.coco_anno_file)\n",
        "\n",
        "    self.img_size = img_size\n",
        "\n",
        "    self.transform = transform\n",
        "\n",
        "    self.init_dataset()\n",
        "\n",
        "  def init_dataset(self):\n",
        "    self.cat_ids = self.coco.getCatIds()\n",
        "\n",
        "    self.img_ids = self.coco.getImgIds()\n",
        "    self.ann_ids = self.coco.getAnnIds(self.img_ids)\n",
        "\n",
        "    print(\"Dataset size {}\".format(len(self.img_ids)))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.img_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_meta = self.coco.loadImgs(self.img_ids[idx])[0]\n",
        "    img_path = os.path.join(self.coco_imgs_dir, img_meta['file_name'])\n",
        "    img = Image.open(img_path)\n",
        "    ann_id = self.coco.getAnnIds(self.img_ids[idx])\n",
        "    annos = self.coco.loadAnns(ann_id)[0]\n",
        "\n",
        "    original_width, original_height = img.size\n",
        "\n",
        "    img = img.resize(self.img_size)\n",
        "    img = np.array(img)\n",
        "\n",
        "    if len(img.shape) == 2:\n",
        "      img = np.expand_dims(img, axis=2)\n",
        "      img = np.repeat(img, 3, axis=2)\n",
        "\n",
        "    img = to_tensor(img)\n",
        "\n",
        "    if self.transform:\n",
        "      img = self.transform(img)\n",
        "\n",
        "    bbox = annos['bbox']  # box is xywh\n",
        "    cat_id = annos['category_id']\n",
        "\n",
        "    x1, y1, w, h = bbox\n",
        "    x2, y2 = x1 + w, y1 + h\n",
        "\n",
        "    x1 = x1 / original_width\n",
        "    x2 = x2 / original_width\n",
        "    y1 = y1 / original_height\n",
        "    y2 = y2 / original_height\n",
        "\n",
        "    coordinates = np.array([x1, y1, x2, y2])\n",
        "    coordinates =  coordinates.astype(np.float32)\n",
        "\n",
        "    return img, coordinates, cat_id"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms \n",
        "\n",
        "transforms = torch.nn.Sequential(\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0),\n",
        ")"
      ],
      "metadata": {
        "id": "GQM1fGOFRaj2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}